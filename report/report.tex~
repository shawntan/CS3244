
\title{CS3244 Machine Learning \\ Assignment 2}
\author{Shawn Tan (U096883L)}
\date{}
\documentclass[12pt]{article}
	\addtolength{\oddsidemargin}{-0.7in}
	\addtolength{\evensidemargin}{-0.7in}
	\addtolength{\textwidth}{1.2in}
	\addtolength{\topmargin}{-0.5in}
	\addtolength{\textheight}{1.5in}
\usepackage[compact]{titlesec}
\usepackage{amsmath}
\usepackage{url}
%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}

\begin{document}
\maketitle
\section{Introduction}
Text classification is a common problem in the field of machine learning and Natural Language Processing (NLP). In this assignment, we were tasked to classify some posts on several newsgroups.

We were given stemmed texts from 5 newsgroups: \url{comp.graphics}, \url{comp.os.ms-windows.misc}, \url{comp.sys.ibm.pc.hardware}, \url{comp.sys.mac.hardware} and \url{comp.windows.x}. In our test set, we were given 1425 instances to classify, and 2935 training instances. Several scripts and programs were supplied to perform various tasks:
\begin{description}
	\item[fs.php and fe.php] These two PHP scripts help to extract the features from the texts using \textsc{TF-IDF} and $\chi^2$ feature selection methods.
	\item[Formatting.exe] This program converts the \url{.txt} files created by the PHP scripts into \url{.arff} files which can be read by WEKA.
\end{description}
The end result are two \url{.arff} files that consist of features that correspond to normalised word frequencies. These are the feature vectors which the various classifiers used will be working with. In this report, we experiment with using 3 different types of classifcation algorithms: $k$-Nearest Neighbour, Naive Bayes, and SVMs. 

Our approach involves training different classifiers using each of the algorithms using the same dataset. Eventually, we take the best performing classifier from each different algorithm, and use these classifiers together to hopefully reduce any kind of overfitting caused by any of the individual algorithms. After evaluating this classifier, we then use this to classify our test set.

We make use of version 3.7.4 of WEKA for the tasks detailed in this report.

\section{Selecting the Features}
 Setting an overly high value for feature selection may result in feature vectors that are too specific to the training set, and eventually cause overfitting. For our first experiment, we select only the top 50 keywords for each class for our feature vector. This resulted in 203 keywords in total.

Using the selected features, we extract the feature vectors from each of the newsgroup posts. Using this, we train three classifiers ($k$NN, Naive Bayes, SVM) using the default WEKA settings, and evaluate their performances before proceeding. We do this several times, with several different values of \url{fs_top_num}. Table \ref{table:fs} reports the different values we tried, and the weighted F-measure of the corresponding classifiers.
\begin{table}[h]
\label{table:fs}
\centering
\begin{tabular}{|l|c| c c c |}
\hline 
	\url{fs_top_num} & Keywords/Features   & \textbf{Naive Bayes}& \textbf{SVM} & \textbf{IBk} \\
\hline
	50	& 203	& 0.738 & 0.77 	& 0.732 \\
	100	& 428   & 0.74	& 0.801	& 0.758	\\
	150 & 641	& 0.743 & 0.814 & 0.767 \\
	200 & 857	& 0.74	& 0.822 & 0.774 \\
\hline
\end{tabular}
\caption{Experiments with the number of features used.}
\end{table}

Increasing \url{fs_top_num} by 50 at each round of testing, we performed the experiment four times. We decided to use an \url{fs_top_num} value of 200 for our classifcation task, as larger feature vectors may cause classification to take long periods of time, making repeated testing difficult. 	
	
	



\section{Tuning Performance of Individual Classifiers}
In the following section we attempt to tune the performance of individual classifiers by adjusting the parameters for the different algorithms. For each algorithm, we evaluate the classifiers based on their performance on the training set and see how the classifiers can be improved.
\subsection{Naive Bayes (\url{NaiveBayes})}
The WEKA implementation of the Naive Bayes classifier does not allow for much tweaking of parameters. The default (using WEKA's settings) classifier has an  F-measure of 0.74 on the training set. It is worth noting that the F-measure on class 4 was 0.809 and has a true positive rate of 0.81 for class 3 , as this may be useful in our attempt to combine the classifiers later on.

\subsection{Support Vector Machines (\url{SMO})}
By default, WEKA's SMO algorithm learns a classifier with a linear decision boundary. By setting the \url{-K} parameter, we can change this to higher exponents. This is effectively mapping the original representation of data points to a different feature space. This is expected to give us better results, since many real-world problems are unlikely to be linearly separated.

The original weighted F-measure for the SVM is 0.822. Again, we see that the F-measure for class 4 is the highest, but its true positive rate for class 0 documents is at 0.86.

We ran the training set against the SMO algorithm with the \url{PolyKernel} at different exponents and evaluated the results.
\begin{table}[h]
\centering
\begin{tabular}{|l |l|c c c c c|}
\hline
Label & Exponent	&	TP Rate & FP Rate & Precision & Recall  & F-Measure	 \\
\hline
\url{SVM} & 1	&		0.821 &    0.045 &     0.823   &   0.821   &   0.822  \\
\url{SVM0.5} & 0.5 &	0.748 &    0.063 &     0.75    &   0.748   &   0.748  \\   
\url{SVM1.5} & 1.5	&	0.844 &    0.039 &     0.845   &   0.844   &   0.844  \\
\url{SVM2.0} & 2.0	&	0.838 &    0.041 &     0.839   &   0.838   &   0.838  \\
\url{SVM3.0} & 3.0	&	0.831 &    0.042 &     0.833   &   0.831   &   0.832  \\
\url{SVMRBF} & $-$	&	0.751 &    0.063 &     0.787   &   0.751   &   0.749  \\

\hline
\end{tabular}
\caption{Experiments with the number of features used.}
\label{table:svm}
\end{table}

Evaluating kernels of exponent 0.5, 1, 1.5, 2, and 3, we saw that the best performing SVM was when \url{-E} was set at 1.5, which performed slightly better than the default settings. This setting brings up the F-measure over all classes to over 0.8, which hopefully, gives us better performance. The best reported F-measure was for class 3 at 0.869. Higher orders of exponents seem to result in poorer results, suggesting that the data does not fit well to functions of orders 2 and above. %try with 0.5

We attempt the same classification task with the RBF kernel, but obtained results poorer than the default settings for the SVM. Again, this suggests that the RBF kernel is not a good fit for the data.

Eventually, we decided to use \url{SVM1.5} for our SVM classifier. Our experimental results are shown in Table \ref{table:svm}. These are listed as the weighted averages of the different shown values.
\subsection{$k$-Nearest Neighbours (\url{IBk})}
The default setting for IBk on WEKA has the $k=1$. This results in every new instance being classified the same a the first nearest neighbour it sees. We experiment with different values of $k$ to find a good classifier.

The classifier trained with the default settings gave an F-measure of 0.774, and an F-measure of 0.805 on class 0 and 0.807 on class 4. From the performance of the other classifiers on this classification, this suggests that instances in classes 0 and 4 are more easily distinguishable from the rest of the dataset.

We experiment with different values of $k$, evaluating the classifier with different $k$ values.
\begin{table}[h]
\centering
\begin{tabular}{|l |l|c c c c c|}
\hline
Label & $k$ &	TP Rate & FP Rate & Precision & Recall  & F-Measure	 \\
\hline
\url{IB1}	& 1		&0.774    &  0.056   &    0.775  &    0.774   &   0.774	  \\
\url{IB5}	& 5 	&0.762    &  0.06    &    0.77   &    0.762   &   0.763	  \\   
\url{IB10}  & 10	&0.768    &  0.058   &    0.775  &    0.768   &   0.768	  \\
\url{IB20}	& 20	&0.769    &  0.058   &    0.776  &    0.769   &   0.77 	  \\
\url{IB30}	& 30	&0.76     &  0.06    &    0.767  &    0.76    &   0.761	  \\

\hline
\end{tabular}
\caption{Experiments with the number of features used.}
\label{table:knn}
\end{table}

Using the \url{-X} function to choose $k$ did not seem to contribute much to the results of the classifier. One noticeable characteristic of this classifier was that it was much faster than the training times of the SVM. However, the results of the classifiers tend to be poorer than that of the SVM as well. We also tried several iterations with the \url{-I} and \url{-F} parameters, which alter the weighting function so that the classifier behaves slightly differently, but these did not improve the performance of the overall classifier much.

It is worth noting that the IBk classifiers generally have nearly 0\% errors when run on their own training set. This is because the points to be classified fall directly on themselves, giving the same results. As such, testing the IBk classifier on the same set of data as the training set is unproductive.


\section{Combining the Classifiers}
From each type of classifier, we pick the two best. We then combine the 5 resulting classifiers into one using the \url{weka.classifiers.meta.Vote} classifier. We choose to combine them by majority vote, which means that the classification will be determined by the most common classification among the 5 classifiers.

Our chosen classifiers are:
\begin{enumerate}
	\item \url{NB}
	\item \url{SVM1.0}
	\item \url{SVM4.0-L}
	\item \url{IB10-I}
	\item \url{IB20-I}
\end{enumerate}
In our selection of the classifiers we have to take into consideration the performance of each of the included classifier. Having multiple classifiers prone to errors would result in a correspondingly error prone combined classifier. The benefit of combining them, however, is due to the fact that the different algorithms have different strengths and weaknesses, and giving only the output which majority of the classifiers agree on is likely to improve the overall performance. The error made by one classifier could then be corrected by the other classifiers.

Combining the classifiers, we run the training set against the newly formed classifier. The obtained results are in Table \ref{table:combi}.

\begin{table}[h]
\centering
\begin{tabular}{|l | c c c c c|}
\hline
Class &	TP Rate & FP Rate & Precision & Recall  & F-Measure	 \\
\hline

\hline
0             &     0.998   &   0   &        0.998   &   0.998   &   0.998  \\  
1             &     0.998   &   0   &        0.998   &   0.998   &   0.998  \\ 
2             &     1       &   0   &        1       &   1       &   1      \\ 
3             &     1       &   0   &        1       &   1       &   1      \\  
4             &     1       &   0   &        1       &   1       &   1      \\  
Weighted Avg. &     0.999   &   0   &        0.999   &   0.999   &   0.999  \\ 
\hline
\end{tabular}
\caption{Class breakdown of the performance of the combined classifier.}
\label{table:combi}
\end{table}

The 


\end{document}




